# Асинхронный парсер документов PEP на базе фреймворка Scrapy

Парсер обрабатывает официальный сайт PEP (https://peps.python.org/) и добывает номера документов PEP, их названия, статусы и количество. Подробнее о формате выдачи ниже.

### Технологии

Проект написан на Python 3.7. Для реализации парсинга выбран фреймворк Scrapy, т. к. он позволяет выполнять код асинхронно и увеличивать таким образом скорость работы программы.

### Установка
Клонируйте репозиторий:

```git clone https://github.com/tanja-ovc/scrapy_parser_pep.git```

Убедитесь, что находитесь в директории _scrapy_parser_pep/_ либо перейдите в неё:

```cd scrapy_parser_pep/```

Cоздайте виртуальное окружение:

```python3 -m venv venv```

Активируйте виртуальное окружение:

Для Mac:
 
```source venv/bin/activate```

Для Windows:

```source venv/Scripts/activate```

При необходимости обновите pip:

```pip install --upgrade pip```

Установите зависимости из файла requirements.txt:

```pip install -r requirements.txt```

Запустите парсер следующей командой:

```scrapy crawl pep```

Результатом работы парсера являются два _.csv_ файла с таким наполнением:
- Первый файл (с названием '*pep_(_дата-и-время-запуска-парсера_).csv*', например *pep_2022-07-06T23-46-38.csv*) содержит список всех PEP в таком формате: номер PEP, его название и статус.
- Второй файл (с названием '*status_summary_(_дата-и-время-запуска-парсера_).csv*', например *status_summary_2022-07-07_02-46-38.csv*) содержит сводку по статусам PEP — сколько найдено документов в каждом статусе (статус, количество). В последней строке этого файла отображено общее количество всех документов ('Total').

Файлы расположены в папке _results_ проекта. В репозиторий загружены два файла для примера.


### Авторство
Автор: Татьяна Овчинникова

Тесты написаны командой Яндекс.Практикума.
